{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "'''⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠂⠠⣦⢶⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⠤⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⡧⠯⠁⠘⣽⡿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡟⠠⢶⣶⣿⣦⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⡛⣩⣵⡂⠀⢐⡒⡆⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀⠨⣵⣻⣧⣽⣳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⢸⡿⣞⡿⣋⡄⠀⠀⢭⣑⢻⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⡇⠀⠀⠔⠫⣿⣿⣿⣿⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣸⣾⣧⣶⠟⡋⣀⡀⠀⢀⣀⣯⢎⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣷⠀⠀⠠⢶⣶⣞⣻⢿⣾⣿⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣈⣾⣫⣿⣽⣶⣯⣟⡶⠄⠀⠀⠲⠄⢧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢹⡆⠀⣴⣓⠚⠛⣿⣿⣿⣾⣿⣿⣷⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡀⠀⠀⠀⠀⣠⣾⣿⣿⣿⣿⣿⣿⡿⠟⠉⠁⠀⠀⠀⠂⡧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⡀⠉⠉⠉⣳⣿⣶⣿⣿⣿⣿⣿⣿⣿⣷⡶⣴⣴⣶⡛⠛⠉⠉⠉⠉⠉⠀⢠⠛⠛⢿⣟⣯⣷⣾⣿⣿⣿⣿⣿⣿⣟⣛⣭⡴⠄⠀⠀⠀⠀⠈⠆⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡧⠀⠀⠈⢉⣉⣿⣿⣿⣿⣿⠿⣛⣿⡿⠀⠀⠀⠻⢿⣦⢀⣀⡀⠀⠀⣤⣇⣀⣼⡞⠈⠉⠉⠉⣀⠉⠛⠻⢿⣿⣿⣟⠓⣦⠔⠀⠀⠀⠀⠀⠨⡕⠅⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣇⠀⣤⣬⣿⣿⠟⠛⠉⠁⠀⠘⣿⣿⠇⠀⠀⠑⠀⠘⠿⠟⠛⠙⠦⡞⠉⠛⠾⠃⠀⠀⠀⠀⠀⢿⣿⣧⠀⠀⠀⠈⠙⢷⣶⣼⡒⠓⠂⠀⢀⠀⠍⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣾⠿⠛⠉⠀⠀⠀⠀⠀⠀⠀⠘⠿⠀⠀⣀⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⠾⡿⠉⠀⠀⠀⠀⠀⠀⠙⠿⣿⣶⡄⠀⢸⡇⡆⠀⠐⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⠏⠉⢀⣰⣷⡿⠷⣾⣶⣶⣤⣀⠀⠀⠀⠀⠹⢦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡰⠈⠀⠀⠀⠁⣀⣤⣶⣾⣽⣿⣷⣤⣄⠀⠉⠛⣧⣿⠽⡀⡅⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡾⠥⠤⠴⣾⡿⠃⢠⣾⣿⡟⠉⠉⣻⡗⢄⠀⠀⠀⠈⠣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⠁⠀⠀⡴⡏⠎⢿⣿⠋⠐⢿⣿⣏⠈⢹⣦⠀⠀⠀⢹⡙⠟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⢏⠀⠀⠀⠀⢿⣇⠀⢸⣿⣿⣧⣀⣰⣿⡇⠀⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⡈⡇⠀⡀⢸⣦⣤⡀⣼⣿⢿⠀⠀⢀⠈⠁⠀⠣⣄⠘⣳⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡼⠃⠀⠁⠄⡀⠀⠀⠛⠦⠌⠿⠿⢿⠿⠿⠋⣠⣾⣿⡀⢀⡤⠖⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡀⠀⢸⣿⣄⡀⠈⠻⢿⢿⠿⡿⠁⣀⡸⠃⠀⠀⠀⣠⠀⠻⢮⢥⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⠁⠀⠀⠀⠀⠀⠀⠃⠰⠠⢄⡀⠀⠀⠀⠀⠀⠀⠀⣨⡿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠿⣷⣜⠋⠉⠉⠉⠉⠓⠀⠐⠀⠈⠈⠄⠀⡄⠐⠈⠁⠁⠀⡁⠂⠁⠘⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠐⠒⠠⢀⠠⢺⡿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢷⣦⡀⠀⠄⠀⠤⠀⠀⠐⠀⠉⠀⠀⠀⠀⠀⠀⠀⠄⠀⠀⢀⢀⡑⠝⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡏⠀⣠⣤⣠⣀⡀⠀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠀⠀⠀⠀⣀⣤⣤⣤⣦⣤⣤⣤⣤⣤⣤⣤⣤⣀⠀⠀⠀⠀⠙⠛⠠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠀⢰⣄⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀⢈⣭⣿⠷⠚⠛⠙⢹⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢿⣿⣿⣿⣿⣿⣽⣿⣻⣟⣿⣽⣻⣟⣿⡷⠀⠀⠀⠀⠀⠐⢡⢀⠀⠀⠀⠀⠀⢀⣶⣴⣦⣤⣤⣄⡀⠀⢀⣶⡧⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣧⠀⠀⠈⠁⠀⠀⠀⠀⢸⣿⡆⠀⠀⠶⠆⢾⠃⠀⠷⠀⠀⠀⠀⠀⠈⠉⠉⢿⣟⣿⣿⣽⣿⡾⣿⡅⠀⠀⠀⠀⠀⠀⠀⢠⡄⢠⢯⠐⣀⠀⢀⣀⡈⠉⠉⠙⠙⠛⠛⠳⢚⡾⣽⡋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⠀⢀⡀⢠⣶⠀⣴⡄⠀⠀⠀⠀⠀⠀⠀⠙⢯⣿⣾⣿⣏⡿⠃⠀⠀⠀⠀⠀⠀⠀⠠⣄⠠⣠⣀⡀⡈⠉⠉⠀⠀⠀⠀⠀⠀⢀⣤⡖⣿⠁⠴⣾⡅⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢧⠀⠀⠀⠀⠀⠀⠀⠀⠹⠃⠀⠀⠀⡀⠀⢀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠙⢿⡿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠉⠐⠀⠀⠀⠀⢠⡴⠞⠋⠁⠀⠀⠀⣠⢂⣥⠡⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢷⣤⣤⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣧⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠀⠈⠈⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⢈⡋⠉⠁⠀⠀⠚⠄⠌⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠏⠡⢺⡿⠏⠀⠀⡠⠀⠀⠀⠀⠀⠀⠐⠒⠠⠤⠄⠠⠤⠤⠤⠴⠒⠚⠉⠉⠉⠉⠉⠉⠓⠲⠤⢤⣀⣀⡀⣀⠤⠒⡖⠊⠩⠉⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⠀⠀⠁⢴⡃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠏⠀⠀⠼⣅⠀⠀⢩⢶⣿⣿⣿⣶⠶⠚⠀⠀⠀⠀⠀⠐⠤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡀⡄⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⡑⠘⡮⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠏⠀⠀⢠⠎⠁⠀⠀⠀⠈⠉⠁⠉⠉⠟⠋⠽⢴⣂⣥⣀⣀⣀⣈⡉⢒⡶⠦⣀⣀⣀⣠⣀⣀⣀⣤⣴⣶⠿⠛⠁⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢬⣿⣡⣗⡅⠤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡏⠀⠀⠀⠀⠀⢡⠔⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠁⠉⠉⠉⠀⠀⠀⠉⠉⠉⡍⠉⠉⢉⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣄⣹⣿⣿⣖⣹⠨⣏⡐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢷⠀⠀⠀⠀⢰⠃⠤⠊⢠⠴⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢹⠀⠀⢸⠐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠤⣄⣤⣄⣹⣿⠛⡓⠂⠀⠙⠀⢻⠬⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⣤⠄⠀⠀⠀⠀⣴⠈⠛⢻⢛⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⠞⠀⠀⠀⠈⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣄⠀⠀⠀⠀⠀⠤⣄⣷⣬⣙⡛⠛⠉⠀⠀⠀⠀⣒⡰⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠃⢼⣿⡇⠀⠀⠀⠀⠈⠁⠀⠟⠉⢩⣭⢉⣠⢀⡤⠀⠀⠀⠀⠀⠀⠀⢀⣾⠇⠀⠀⠀⠀⠀⠀⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣔⢆⢸⡀⠀⢀⣰⣠⡿⠒⠂⠈⠀⠁⠀⠀⠀⢀⠈⢠⣳⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⡄⠀⠿⢟⡿⣶⡀⠀⠀⠀⠀⠀⠀⠸⠋⠡⣰⢿⣧⣞⣡⣾⣀⣄⣤⡾⠋⠈⠀⠀⠀⠀⠀⠀⠀⠀⢳⡈⢿⣆⣠⣀⢠⢀⡀⢠⣬⣏⣳⣬⠿⠛⠃⠈⠉⠀⠀⠀⠀⠀⠀⠀⠀⡰⣟⣿⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⡀⠀⠈⣼⠥⣿⣧⣶⣾⣀⡀⠀⠀⠀⠀⠁⠘⠁⠐⠛⠁⠘⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠃⠀⠉⠉⠛⠿⠿⠟⢻⣿⠉⠛⠙⠒⠒⠀⠀⠀⠀⠀⠀⠀⠀⢈⣷⣾⣾⣧⣎⡯⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⡇⢰⣀⠀⢠⠿⣿⣿⣿⣼⣧⣾⢀⠀⠀⠀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡀⠀⠀⠐⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣝⠒⢶⣿⡿⢿⡏⠁⠼⠁⡋⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢹⣿⣷⡼⣿⣄⠀⠀⠀⢨⠿⢻⣿⣿⣧⣾⣷⣏⣡⢀⣠⣷⣖⣴⣶⣶⣾⠤⢀⣤⣴⡾⠁⠀⠀⠘⣿⡷⣦⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⢰⣶⣀⠰⣿⣷⣾⠏⠉⠀⠸⢦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⣿⣿⢻⠿⠇⡀⠀⠀⠀⠘⠈⠁⠀⡽⠛⠟⠋⣵⣾⡿⠿⠛⡿⠋⠁⡰⢫⢯⢿⠁⠀⠀⠀⠀⠘⠳⣽⣿⣦⢀⣞⣤⡀⠹⣤⣦⠀⡀⢴⢠⠀⢳⣿⣭⣛⠻⠏⠉⠉⠀⠀⠀⡨⢸⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣟⣾⣿⣷⣿⣧⡄⠀⠀⠀⠀⠀⠀⠀⠀⠒⠚⠉⠀⠀⠈⠀⠀⠊⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠻⣿⢿⡿⣿⡦⣼⣿⡳⣽⣽⣷⠷⢬⠉⠉⠉⠁⠀⠀⠀⠀⠀⠀⢀⡨⣠⠴⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⢩⣯⣼⣿⣿⡿⠻⡜⣦⣠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠃⠀⠀⠀⠈⠙⠋⠉⠉⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⢀⠐⣤⣼⣿⢩⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⡇⣾⣽⣿⣿⢟⣵⢿⡇⣻⡿⣇⠀⠀⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠲⣤⣀⠀⠀⠀⠀⠀⠀⠀⢠⣠⢤⡀⠀⠤⠀⠀⠐⠀⠐⡐⣤⣿⣂⣿⠿⣷⠿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⣻⢿⣿⢯⣾⣿⣿⣷⣿⢗⡙⢃⣷⣾⠀⡆⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠇⠀⠑⠆⠀⠀⠀⠀⠲⠌⠉⠁⠁⠀⠀⢠⡀⢤⣐⣈⣿⣿⣿⣿⣿⣿⣿⠦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣳⢯⣿⣿⢏⣾⣿⣿⣿⣾⣼⡿⠁⡿⠏⢻⡸⣇⣶⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⢤⣀⣀⣀⠀⣐⣶⣾⣹⣯⣷⣚⠿⣷⣿⣿⣿⣿⡟⣿⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠇⣾⣿⣿⣿⣿⣿⣿⣿⣿⡟⣁⣼⠟⣐⣬⡷⣠⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢰⣤⣴⣶⣶⠾⣷⣦⣽⡿⢩⣌⠻⣿⣷⣯⣟⡿⣿⣿⣿⣿⣿⣿⠇⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡟⢀⣼⣿⣿⣿⣿⣿⣿⡿⣋⣴⠟⣡⣶⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠿⣿⣷⣹⣿⡷⣿⣦⣻⣿⣿⣿⣿⣿⣿⣾⣿⣿⣿⣿⡿⡏⣯⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢘⡇⠸⣿⢋⣿⠟⠋⢁⡿⢠⠛⠠⠌⠉⣿⡿⣿⣿⡿⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣿⣷⢿⣿⣿⣿⣿⡟⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⢷⣧⣼⡀⠀\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "goa-qwj-r36v",
    "outputId": "3af2a589-f172-413b-f1ec-73743c8ddd02"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Union\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "id": "K3RFfKhvoVJl",
    "ExecuteTime": {
     "end_time": "2025-07-13T12:26:02.205756Z",
     "start_time": "2025-07-13T12:26:02.196983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda"
   ],
   "metadata": {
    "id": "4v1M4Mgaodvi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9f05ec74-9fc9-4dcc-b1d9-4a7091ecd3a9",
    "ExecuteTime": {
     "end_time": "2025-07-13T12:26:18.679365Z",
     "start_time": "2025-07-13T12:26:17.018996Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pathlib\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ],
   "metadata": {
    "collapsed": true,
    "id": "e-59QIjIj06-",
    "ExecuteTime": {
     "end_time": "2025-07-13T12:26:20.191365Z",
     "start_time": "2025-07-13T12:26:20.131671Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": "## Papers download.",
   "metadata": {
    "id": "5GOkA-mjeYBs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "EMAIL = os.getenv(\"EMAIL\", 'trifonova.kate.s@gmail.com')\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "\n",
    "\n",
    "def works_with_oa(query, first_n=1):\n",
    "    params = {\n",
    "        'filter': f'abstract.search:\"{query}\",best_open_version:published',\n",
    "        'per-page': first_n,\n",
    "        'select':'id,title,best_oa_location',\n",
    "        'mailto': EMAIL\n",
    "    }\n",
    "    papers = requests.get(BASE_URL, params=params).json()['results']\n",
    "    return papers\n",
    "\n",
    "def get_pdf(paper: dict, location: str) -> pathlib.Path | None:\n",
    "    url = paper.get('best_oa_location', {}).get('pdf_url')\n",
    "    paper_id = paper['id'].split('/')[-1]\n",
    "    pdf_dir = pathlib.Path(location)\n",
    "    pdf_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_path = pdf_dir / f\"{paper_id}.pdf\"\n",
    "\n",
    "    if not url:\n",
    "        print(f'No PDF URL for paper {paper_id}')\n",
    "        return None\n",
    "\n",
    "    # Skip existing files\n",
    "    if pdf_path.exists():\n",
    "        return pdf_path\n",
    "\n",
    "    # Attempt download with error handling\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update({'From': EMAIL})\n",
    "        try:\n",
    "            resp = session.get(url, stream=True)\n",
    "            resp.raise_for_status()\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"HTTPError downloading PDF for paper {paper_id}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading PDF for paper {paper_id}: {e}\")\n",
    "            return None\n",
    "        # Write content in chunks\n",
    "        with open(pdf_path, 'wb') as f:\n",
    "            for chunk in resp.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    return pdf_path"
   ],
   "metadata": {
    "id": "IepIxS7NdQXx",
    "ExecuteTime": {
     "end_time": "2025-07-13T12:53:24.594522Z",
     "start_time": "2025-07-13T12:53:24.510487Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "def has_text(pdf_path: Union[str, pathlib.Path], max_pages: int = 3, min_words: int = 50) -> bool:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    word_count = 0\n",
    "\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        if i >= max_pages:\n",
    "            break\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            words = text.strip().split()\n",
    "            word_count += len(words)\n",
    "            if word_count >= min_words:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def load_and_split_docs(pdf_path: pathlib.Path) -> list:\n",
    "\n",
    "    if has_text(pdf_path):\n",
    "        loader = PyPDFLoader(str(pdf_path))\n",
    "    else:\n",
    "        loader = UnstructuredPDFLoader(str(pdf_path), strategy=\"ocr_only\")\n",
    "\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = splitter.split_documents(docs)\n",
    "\n",
    "    for i, doc in enumerate(splits):\n",
    "        doc.metadata.update({\n",
    "            \"chunk_index\": i,\n",
    "            \"source_path\": str(pdf_path)\n",
    "        })\n",
    "\n",
    "    return splits"
   ],
   "metadata": {
    "id": "u3vG0s4SfEzv",
    "ExecuteTime": {
     "end_time": "2025-07-13T12:53:24.653070Z",
     "start_time": "2025-07-13T12:53:24.633716Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "def fetch_and_prepare(query: str, location: str = './pdfs', first_new: int = 3) -> list:\n",
    "\n",
    "    pdf_dir = pathlib.Path(location)\n",
    "    pdf_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Over-fetch candidates (3x) to allow skipping existing files\n",
    "    candidates = works_with_oa(query, first_new * 3)\n",
    "    new_docs = []\n",
    "\n",
    "    for paper in candidates:\n",
    "        paper_id = paper['id'].split('/')[-1]\n",
    "        pdf_path = pdf_dir / f\"{paper_id}.pdf\"\n",
    "        # Skip if already downloaded\n",
    "        if pdf_path.exists():\n",
    "            continue\n",
    "        # Download and prepare\n",
    "        downloaded = get_pdf(paper, location)\n",
    "        if downloaded:\n",
    "            new_docs.extend(load_and_split_docs(downloaded))\n",
    "        # Stop once we've got enough new ones\n",
    "        if len(new_docs) >= first_new:\n",
    "            break\n",
    "    return new_docs"
   ],
   "metadata": {
    "id": "iFVsjrCYdYJM",
    "ExecuteTime": {
     "end_time": "2025-07-13T12:53:24.722771Z",
     "start_time": "2025-07-13T12:53:24.706575Z"
    }
   },
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "#the way to use rerun.\n",
    "\n",
    "# chain = rerun_pipeline('panthera leo', first_new=5)\n",
    "# answer = chain.invoke(\"question: What is a jaguar?\")\n",
    "# print(answer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjoFUlcSkQlF",
    "outputId": "8e6ab870-9842-4438-d90e-e899b299efa9",
    "ExecuteTime": {
     "end_time": "2025-07-13T12:53:24.753422Z",
     "start_time": "2025-07-13T12:53:24.749372Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "# what is the point of this?\n",
    "\n",
    "#what is the point of anything at all?\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#change its name.\n",
    "def function1(query: str, prompt: str):\n",
    "\n",
    "  embedding = OpenAIEmbeddings()\n",
    "\n",
    "  splits = fetch_and_prepare(query)\n",
    "\n",
    "  vectorstore = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "  retriever = vectorstore.as_retriever()\n",
    "\n",
    "  prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "  rag_chain = (\n",
    "      {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "      | prompt\n",
    "      | llm\n",
    "      | StrOutputParser()\n",
    "  )\n",
    "\n",
    "  answer = rag_chain.invoke({\"question\": prompt})\n",
    "\n",
    "  return answer"
   ],
   "metadata": {
    "id": "NYK3i1Vjrw_W",
    "ExecuteTime": {
     "end_time": "2025-07-13T12:53:25.590984Z",
     "start_time": "2025-07-13T12:53:25.584821Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "id": "wGAtiVXjkDfc",
    "ExecuteTime": {
     "end_time": "2025-07-13T12:53:25.978118Z",
     "start_time": "2025-07-13T12:53:25.973639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_default_first_new = 10\n",
    "_default_location = './pdfs'\n",
    "\n",
    "#since we have promt and llm defined inside of a function this pice of code doesnt work. we should move to classes approach.\n",
    "\n",
    "# def rerun_pipeline(query: str, first_new: int = _default_first_new, location: str = _default_location):\n",
    "#     splits = fetch_and_prepare(query, location, first_new)\n",
    "#     if not splits:\n",
    "#         raise ValueError(f\"No new document chunks retrieved for query '{query}'.\")\n",
    "#     vectorstore = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "#     retriever = vectorstore.as_retriever()\n",
    "#     return (\n",
    "#         {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "#         | prompt\n",
    "#         | llm\n",
    "#         | StrOutputParser()\n",
    "#     )\n"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "id": "qG1DQ5YNumJj",
    "ExecuteTime": {
     "end_time": "2025-07-13T13:22:00.886253Z",
     "start_time": "2025-07-13T13:22:00.726960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RAG:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vectorstore = Chroma(collection_name=\"bunch_of_docs\", embedding_function=OpenAIEmbeddings())\n",
    "        self.threshold = 0.5\n",
    "\n",
    "    @staticmethod\n",
    "    def has_text(pdf_path: str, max_pages: int = 3, min_words: int = 10) -> bool:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        word_count = 0\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            if i >= max_pages:\n",
    "                break\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                words = text.strip().split()\n",
    "                word_count += len(words)\n",
    "                if word_count >= min_words:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_and_prepare(query: str, location: str = './pdfs', first_new: int = 10) -> list:\n",
    "\n",
    "        pdf_dir = pathlib.Path(location)\n",
    "        pdf_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Over-fetch candidates (3x) to allow skipping existing files\n",
    "        candidates = works_with_oa(query, first_new * 3)\n",
    "        downloaded_paths = []\n",
    "\n",
    "        for paper in candidates:\n",
    "            paper_id = paper['id'].split('/')[-1]\n",
    "            pdf_path = pdf_dir / f\"{paper_id}.pdf\"\n",
    "\n",
    "            # Skip if already downloaded\n",
    "            if pdf_path.exists():\n",
    "                continue\n",
    "            # how to do this without RAG()\n",
    "            try:\n",
    "                downloaded = get_pdf(paper, location)\n",
    "                if downloaded:\n",
    "                    downloaded_paths.append(downloaded)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to download PDF for paper {paper['id']}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if len(downloaded_paths) >= first_new:\n",
    "                break\n",
    "\n",
    "        return downloaded_paths\n",
    "\n",
    "    def load_ocr(self, pdf_path: str, puppy: str = None):\n",
    "        if self.has_text(pdf_path):\n",
    "            print(\"Text detected in PDF. Using fast strategy.\")\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "        else:\n",
    "            print(\"No text detected. Using OCR.\")\n",
    "            loader = UnstructuredPDFLoader(pdf_path, strategy=\"ocr_only\")\n",
    "\n",
    "        docs = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = splitter.split_documents(docs)\n",
    "\n",
    "        for i, doc in enumerate(splits):\n",
    "            doc.metadata.update({\n",
    "                \"chunk_index\": i,\n",
    "                \"source_path\": str(pdf_path),\n",
    "                \"specie\": puppy\n",
    "            })\n",
    "\n",
    "        self.vectorstore.add_documents(splits)\n",
    "        return splits\n",
    "\n",
    "    def chain(self, question):\n",
    "        if not self.vectorstore:\n",
    "            return \"Vector store not initialized.\"\n",
    "\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "        results_with_scores = self.vectorstore.similarity_search_with_score(question, k=20)\n",
    "        filtered_docs = [doc for doc, score in results_with_scores if score <= self.threshold]\n",
    "\n",
    "        if not filtered_docs:\n",
    "            return \"booooooo, no docs found within the given threshold\"\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=pathlib.Path('prompt.txt').read_text(encoding='utf8')\n",
    "        )\n",
    "\n",
    "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "        def inspect_prompt(inputs):\n",
    "            formatted_prompt = prompt.format(context=format_docs(filtered_docs), question=question)\n",
    "            print(formatted_prompt)\n",
    "            return inputs\n",
    "\n",
    "        rag_chain = (\n",
    "            {\"context\": RunnableLambda(lambda _: format_docs(filtered_docs)),\n",
    "             \"question\": RunnablePassthrough()}\n",
    "            | RunnableLambda(inspect_prompt)\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        return rag_chain.invoke(question)\n",
    "\n",
    "    def query(self, question: str):\n",
    "        for puppy in question.split(\",\"):\n",
    "            pdf_paths = self.fetch_and_prepare(query=puppy)\n",
    "\n",
    "            for pdf_path in pdf_paths:\n",
    "                print(f'Processing PDF: {pdf_path}')\n",
    "                self.load_ocr(pdf_path=pdf_path, puppy=puppy.strip())\n",
    "\n",
    "        print(self.chain(question))"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T13:22:03.773505Z",
     "start_time": "2025-07-13T13:22:03.571106Z"
    }
   },
   "cell_type": "code",
   "source": "RAG_obj = RAG()",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T13:24:22.719319Z",
     "start_time": "2025-07-13T13:22:20.073570Z"
    }
   },
   "cell_type": "code",
   "source": "RAG_obj.query(\"Canis lupus, Salmo salar\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PDF URL for paper W1835981228\n",
      "HTTPError downloading PDF for paper W2174884086: 403 Client Error: Forbidden for url: https://academic.oup.com/bioscience/article-pdf/54/8/755/26895925/54-8-755.pdf\n",
      "HTTPError downloading PDF for paper W2074807536: 403 Client Error: Forbidden for url: https://cdnsciencepub.com/doi/pdf/10.1139/z01-094\n",
      "No PDF URL for paper W2054065543\n",
      "No PDF URL for paper W2031395929\n",
      "HTTPError downloading PDF for paper W2747773696: 403 Client Error: Forbidden for url: https://besjournals.onlinelibrary.wiley.com/doi/pdfdirect/10.1111/2041-210X.12871\n",
      "HTTPError downloading PDF for paper W2171358256: 403 Client Error: Forbidden for url: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1046/j.1365-2656.2003.00766.x\n",
      "HTTPError downloading PDF for paper W1487352212: 403 Client Error: Forbidden for url: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1365-2664.2008.01466.x\n",
      "HTTPError downloading PDF for paper W2474287676: 403 Client Error: Forbidden for url: https://besjournals.onlinelibrary.wiley.com/doi/pdfdirect/10.1111/1365-2664.12732\n",
      "No PDF URL for paper W2085467573\n",
      "HTTPError downloading PDF for paper W2115332643: 403 Client Error: Forbidden for url: https://meridian.allenpress.com/naf/article-pdf/doi/10.3996/nafa.77.0001/2522191/nafa.77.0001.pdf\n",
      "HTTPError downloading PDF for paper W2154140686: 403 Client Error: Forbidden for url: https://royalsocietypublishing.org/doi/pdf/10.1098/rsfs.2011.0086\n",
      "HTTPError downloading PDF for paper W2775674806: 403 Client Error: Forbidden for url: https://esajournals.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/ecm.1313\n",
      "HTTPError downloading PDF for paper W1767177626: 403 Client Error: Forbidden for url: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/cobi.12009\n",
      "HTTPError downloading PDF for paper W2174346498: 403 Client Error: Forbidden for url: https://academic.oup.com/jmammal/article-pdf/86/2/255/7024345/86-2-255.pdf\n",
      "HTTPError downloading PDF for paper W2173020002: 403 Client Error: Forbidden for url: https://academic.oup.com/jmammal/article-pdf/84/1/243/7024082/84-1-243.pdf\n",
      "HTTPError downloading PDF for paper W2111629801: 403 Client Error: Forbidden for url: https://academic.oup.com/mbe/article-pdf/31/5/1200/13171325/msu070.pdf\n",
      "HTTPError downloading PDF for paper W2496750754: 403 Client Error: Forbidden for url: https://academic.oup.com/beheco/article-pdf/27/6/1826/8972346/arw117.pdf\n",
      "eto put\n",
      "pdfs/W2146381121.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2111674457.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2111209696.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2156313088.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2158169942.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W1967686316.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2118945051.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2119757357.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2198254715.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2724105174.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "HTTPError downloading PDF for paper W2045251542: 403 Client Error: Forbidden for url: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/j.1095-8649.2010.02762.x\n",
      "No PDF URL for paper W1987228708\n",
      "HTTPError downloading PDF for paper W1974073103: 401 Client Error: Unauthorized for url: https://www.int-res.com/articles/dao/41/d041p043.pdf\n",
      "No PDF URL for paper W2903897186\n",
      "No PDF URL for paper W2023266467\n",
      "No PDF URL for paper W1988695122\n",
      "No PDF URL for paper W2165742276\n",
      "HTTPError downloading PDF for paper W1977451416: 401 Client Error: Unauthorized for url: https://www.int-res.com/articles/dao/29/d029p219.pdf\n",
      "HTTPError downloading PDF for paper W2592593962: 403 Client Error: Forbidden for url: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/faf.12214\n",
      "No PDF URL for paper W2151721329\n",
      "No PDF URL for paper W2015371172\n",
      "HTTPError downloading PDF for paper W2098073174: 403 Client Error: Forbidden for url: https://academic.oup.com/icesjms/article-pdf/69/9/1538/29143907/fss013.pdf\n",
      "HTTPError downloading PDF for paper W2943192541: 403 Client Error: Forbidden for url: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/raq.12337\n",
      "HTTPError downloading PDF for paper W2013626668: 401 Client Error: Unauthorized for url: https://www.int-res.com/articles/dao/22/d022p025.pdf\n",
      "HTTPError downloading PDF for paper W2593775995: 403 Client Error: Forbidden for url: https://academic.oup.com/icesjms/article-pdf/74/6/1496/28658315/fsx020.pdf\n",
      "No PDF URL for paper W2765860769\n",
      "eto put\n",
      "pdfs/W2337283254.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2105135948.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2234983292.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2134256281.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2136490136.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2158737312.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2158240881.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2107993129.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W2166504915.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "eto put\n",
      "pdfs/W1970068443.pdf\n",
      "Text detected in PDF. Using fast strategy.\n",
      "You are an experienced zoologist.\n",
      "You are going to be given a number of animal species for comparing analysis. When comparing species, list only COMMON morphological, behavioral, or ecological features\n",
      "present in the INDICATED taxa. Avoid anthropomorphism.\n",
      "Use concrete, specific phrases (e.g. “Long snout”, “Powerful digging claws”).\n",
      "Do NOT use abstract category words such as adaptation(s), feature(s), trait(s),\n",
      "behaviour, ecology, habitat, habits, reproduction.\n",
      "If any forbidden word appears, or if you add headings, comments or blank lines,\n",
      "the answer is invalid.\n",
      "\n",
      "Output format requirements:\n",
      "• List exactly 5 points that demonstrate unique COMMON traits for the NAMED species and affect various aspects of their lives.\n",
      "• Each bullet = 3–5 words, no punctuation at end, use more scientific terms, but not narrowly specialized\n",
      "• Start directly with the first bullet; no headings or extra text.\n",
      "• Give examples that are characteristic only of these NAMED species and NOT characteristic of other closely related species, orders, families, genera\n",
      "• No headings, blank lines, or extra comments of any kind.\n",
      "Begin immediately with the first bullet.\n",
      "1. Prehensile tail for grasping branches\n",
      "2. Opposable thumbs for gripping objects\n",
      "3. Forward-facing eyes for depth perception\n",
      "4. Arboreal lifestyle for living in trees\n",
      "5. Omnivorous diet for varied nutrition\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T12:19:20.051994Z",
     "start_time": "2025-07-13T12:19:20.050293Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T12:19:20.075913Z",
     "start_time": "2025-07-13T12:19:20.074618Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
