{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "'''⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡠⠂⠠⣦⢶⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⠤⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⡧⠯⠁⠘⣽⡿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡟⠠⢶⣶⣿⣦⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣴⡛⣩⣵⡂⠀⢐⡒⡆⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡇⠀⠨⣵⣻⣧⣽⣳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⢸⡿⣞⡿⣋⡄⠀⠀⢭⣑⢻⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⡇⠀⠀⠔⠫⣿⣿⣿⣿⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣸⣾⣧⣶⠟⡋⣀⡀⠀⢀⣀⣯⢎⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣷⠀⠀⠠⢶⣶⣞⣻⢿⣾⣿⣷⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣈⣾⣫⣿⣽⣶⣯⣟⡶⠄⠀⠀⠲⠄⢧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢹⡆⠀⣴⣓⠚⠛⣿⣿⣿⣾⣿⣿⣷⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣀⣀⣀⣀⣀⣀⡀⠀⠀⠀⠀⣠⣾⣿⣿⣿⣿⣿⣿⡿⠟⠉⠁⠀⠀⠀⠂⡧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⡀⠉⠉⠉⣳⣿⣶⣿⣿⣿⣿⣿⣿⣿⣷⡶⣴⣴⣶⡛⠛⠉⠉⠉⠉⠉⠀⢠⠛⠛⢿⣟⣯⣷⣾⣿⣿⣿⣿⣿⣿⣟⣛⣭⡴⠄⠀⠀⠀⠀⠈⠆⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⡧⠀⠀⠈⢉⣉⣿⣿⣿⣿⣿⠿⣛⣿⡿⠀⠀⠀⠻⢿⣦⢀⣀⡀⠀⠀⣤⣇⣀⣼⡞⠈⠉⠉⠉⣀⠉⠛⠻⢿⣿⣿⣟⠓⣦⠔⠀⠀⠀⠀⠀⠨⡕⠅⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣇⠀⣤⣬⣿⣿⠟⠛⠉⠁⠀⠘⣿⣿⠇⠀⠀⠑⠀⠘⠿⠟⠛⠙⠦⡞⠉⠛⠾⠃⠀⠀⠀⠀⠀⢿⣿⣧⠀⠀⠀⠈⠙⢷⣶⣼⡒⠓⠂⠀⢀⠀⠍⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣾⠿⠛⠉⠀⠀⠀⠀⠀⠀⠀⠘⠿⠀⠀⣀⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⠾⡿⠉⠀⠀⠀⠀⠀⠀⠙⠿⣿⣶⡄⠀⢸⡇⡆⠀⠐⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⠏⠉⢀⣰⣷⡿⠷⣾⣶⣶⣤⣀⠀⠀⠀⠀⠹⢦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡰⠈⠀⠀⠀⠁⣀⣤⣶⣾⣽⣿⣷⣤⣄⠀⠉⠛⣧⣿⠽⡀⡅⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡾⠥⠤⠴⣾⡿⠃⢠⣾⣿⡟⠉⠉⣻⡗⢄⠀⠀⠀⠈⠣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⠁⠀⠀⡴⡏⠎⢿⣿⠋⠐⢿⣿⣏⠈⢹⣦⠀⠀⠀⢹⡙⠟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⢏⠀⠀⠀⠀⢿⣇⠀⢸⣿⣿⣧⣀⣰⣿⡇⠀⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⡈⡇⠀⡀⢸⣦⣤⡀⣼⣿⢿⠀⠀⢀⠈⠁⠀⠣⣄⠘⣳⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡼⠃⠀⠁⠄⡀⠀⠀⠛⠦⠌⠿⠿⢿⠿⠿⠋⣠⣾⣿⡀⢀⡤⠖⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡀⠀⢸⣿⣄⡀⠈⠻⢿⢿⠿⡿⠁⣀⡸⠃⠀⠀⠀⣠⠀⠻⢮⢥⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⠁⠀⠀⠀⠀⠀⠀⠃⠰⠠⢄⡀⠀⠀⠀⠀⠀⠀⠀⣨⡿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠿⣷⣜⠋⠉⠉⠉⠉⠓⠀⠐⠀⠈⠈⠄⠀⡄⠐⠈⠁⠁⠀⡁⠂⠁⠘⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠐⠒⠠⢀⠠⢺⡿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⢷⣦⡀⠀⠄⠀⠤⠀⠀⠐⠀⠉⠀⠀⠀⠀⠀⠀⠀⠄⠀⠀⢀⢀⡑⠝⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡏⠀⣠⣤⣠⣀⡀⠀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠀⠀⠀⠀⣀⣤⣤⣤⣦⣤⣤⣤⣤⣤⣤⣤⣤⣀⠀⠀⠀⠀⠙⠛⠠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡠⠀⢰⣄⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⠀⢈⣭⣿⠷⠚⠛⠙⢹⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢿⣿⣿⣿⣿⣿⣽⣿⣻⣟⣿⣽⣻⣟⣿⡷⠀⠀⠀⠀⠀⠐⢡⢀⠀⠀⠀⠀⠀⢀⣶⣴⣦⣤⣤⣄⡀⠀⢀⣶⡧⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣧⠀⠀⠈⠁⠀⠀⠀⠀⢸⣿⡆⠀⠀⠶⠆⢾⠃⠀⠷⠀⠀⠀⠀⠀⠈⠉⠉⢿⣟⣿⣿⣽⣿⡾⣿⡅⠀⠀⠀⠀⠀⠀⠀⢠⡄⢠⢯⠐⣀⠀⢀⣀⡈⠉⠉⠙⠙⠛⠛⠳⢚⡾⣽⡋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⡇⠀⠀⢀⡀⢠⣶⠀⣴⡄⠀⠀⠀⠀⠀⠀⠀⠙⢯⣿⣾⣿⣏⡿⠃⠀⠀⠀⠀⠀⠀⠀⠠⣄⠠⣠⣀⡀⡈⠉⠉⠀⠀⠀⠀⠀⠀⢀⣤⡖⣿⠁⠴⣾⡅⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢧⠀⠀⠀⠀⠀⠀⠀⠀⠹⠃⠀⠀⠀⡀⠀⢀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠙⢿⡿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠉⠐⠀⠀⠀⠀⢠⡴⠞⠋⠁⠀⠀⠀⣠⢂⣥⠡⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢷⣤⣤⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣧⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠀⠈⠈⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠤⢈⡋⠉⠁⠀⠀⠚⠄⠌⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠏⠡⢺⡿⠏⠀⠀⡠⠀⠀⠀⠀⠀⠀⠐⠒⠠⠤⠄⠠⠤⠤⠤⠴⠒⠚⠉⠉⠉⠉⠉⠉⠓⠲⠤⢤⣀⣀⡀⣀⠤⠒⡖⠊⠩⠉⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⠀⠀⠁⢴⡃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠏⠀⠀⠼⣅⠀⠀⢩⢶⣿⣿⣿⣶⠶⠚⠀⠀⠀⠀⠀⠐⠤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡀⡄⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⡑⠘⡮⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠏⠀⠀⢠⠎⠁⠀⠀⠀⠈⠉⠁⠉⠉⠟⠋⠽⢴⣂⣥⣀⣀⣀⣈⡉⢒⡶⠦⣀⣀⣀⣠⣀⣀⣀⣤⣴⣶⠿⠛⠁⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢬⣿⣡⣗⡅⠤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡏⠀⠀⠀⠀⠀⢡⠔⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠁⠉⠉⠉⠀⠀⠀⠉⠉⠉⡍⠉⠉⢉⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣄⣹⣿⣿⣖⣹⠨⣏⡐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢷⠀⠀⠀⠀⢰⠃⠤⠊⢠⠴⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢹⠀⠀⢸⠐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠤⣄⣤⣄⣹⣿⠛⡓⠂⠀⠙⠀⢻⠬⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⣤⠄⠀⠀⠀⠀⣴⠈⠛⢻⢛⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⠞⠀⠀⠀⠈⣿⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣄⠀⠀⠀⠀⠀⠤⣄⣷⣬⣙⡛⠛⠉⠀⠀⠀⠀⣒⡰⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⠃⢼⣿⡇⠀⠀⠀⠀⠈⠁⠀⠟⠉⢩⣭⢉⣠⢀⡤⠀⠀⠀⠀⠀⠀⠀⢀⣾⠇⠀⠀⠀⠀⠀⠀⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣔⢆⢸⡀⠀⢀⣰⣠⡿⠒⠂⠈⠀⠁⠀⠀⠀⢀⠈⢠⣳⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⡄⠀⠿⢟⡿⣶⡀⠀⠀⠀⠀⠀⠀⠸⠋⠡⣰⢿⣧⣞⣡⣾⣀⣄⣤⡾⠋⠈⠀⠀⠀⠀⠀⠀⠀⠀⢳⡈⢿⣆⣠⣀⢠⢀⡀⢠⣬⣏⣳⣬⠿⠛⠃⠈⠉⠀⠀⠀⠀⠀⠀⠀⠀⡰⣟⣿⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⡀⠀⠈⣼⠥⣿⣧⣶⣾⣀⡀⠀⠀⠀⠀⠁⠘⠁⠐⠛⠁⠘⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠃⠀⠉⠉⠛⠿⠿⠟⢻⣿⠉⠛⠙⠒⠒⠀⠀⠀⠀⠀⠀⠀⠀⢈⣷⣾⣾⣧⣎⡯⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⡇⢰⣀⠀⢠⠿⣿⣿⣿⣼⣧⣾⢀⠀⠀⠀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡀⠀⠀⠐⣆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣝⠒⢶⣿⡿⢿⡏⠁⠼⠁⡋⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢹⣿⣷⡼⣿⣄⠀⠀⠀⢨⠿⢻⣿⣿⣧⣾⣷⣏⣡⢀⣠⣷⣖⣴⣶⣶⣾⠤⢀⣤⣴⡾⠁⠀⠀⠘⣿⡷⣦⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⢰⣶⣀⠰⣿⣷⣾⠏⠉⠀⠸⢦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⣿⣿⢻⠿⠇⡀⠀⠀⠀⠘⠈⠁⠀⡽⠛⠟⠋⣵⣾⡿⠿⠛⡿⠋⠁⡰⢫⢯⢿⠁⠀⠀⠀⠀⠘⠳⣽⣿⣦⢀⣞⣤⡀⠹⣤⣦⠀⡀⢴⢠⠀⢳⣿⣭⣛⠻⠏⠉⠉⠀⠀⠀⡨⢸⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣟⣾⣿⣷⣿⣧⡄⠀⠀⠀⠀⠀⠀⠀⠀⠒⠚⠉⠀⠀⠈⠀⠀⠊⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠻⣿⢿⡿⣿⡦⣼⣿⡳⣽⣽⣷⠷⢬⠉⠉⠉⠁⠀⠀⠀⠀⠀⠀⢀⡨⣠⠴⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⢩⣯⣼⣿⣿⡿⠻⡜⣦⣠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠃⠀⠀⠀⠈⠙⠋⠉⠉⠋⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⢀⠐⣤⣼⣿⢩⡏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⡇⣾⣽⣿⣿⢟⣵⢿⡇⣻⡿⣇⠀⠀⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠲⣤⣀⠀⠀⠀⠀⠀⠀⠀⢠⣠⢤⡀⠀⠤⠀⠀⠐⠀⠐⡐⣤⣿⣂⣿⠿⣷⠿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢿⣻⢿⣿⢯⣾⣿⣿⣷⣿⢗⡙⢃⣷⣾⠀⡆⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠇⠀⠑⠆⠀⠀⠀⠀⠲⠌⠉⠁⠁⠀⠀⢠⡀⢤⣐⣈⣿⣿⣿⣿⣿⣿⣿⠦⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣳⢯⣿⣿⢏⣾⣿⣿⣿⣾⣼⡿⠁⡿⠏⢻⡸⣇⣶⣤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⢤⣀⣀⣀⠀⣐⣶⣾⣹⣯⣷⣚⠿⣷⣿⣿⣿⣿⡟⣿⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠇⣾⣿⣿⣿⣿⣿⣿⣿⣿⡟⣁⣼⠟⣐⣬⡷⣠⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠐⢰⣤⣴⣶⣶⠾⣷⣦⣽⡿⢩⣌⠻⣿⣷⣯⣟⡿⣿⣿⣿⣿⣿⣿⠇⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡟⢀⣼⣿⣿⣿⣿⣿⣿⡿⣋⣴⠟⣡⣶⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠿⣿⣷⣹⣿⡷⣿⣦⣻⣿⣿⣿⣿⣿⣿⣾⣿⣿⣿⣿⡿⡏⣯⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢘⡇⠸⣿⢋⣿⠟⠋⢁⡿⢠⠛⠠⠌⠉⣿⡿⣿⣿⡿⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣿⣷⢿⣿⣿⣿⣿⡟⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⢷⣧⣼⡀⠀\n",
    "\n",
    "'''"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "goa-qwj-r36v",
    "outputId": "3af2a589-f172-413b-f1ec-73743c8ddd02"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import Union\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "id": "K3RFfKhvoVJl",
    "ExecuteTime": {
     "end_time": "2025-07-13T06:43:15.589579Z",
     "start_time": "2025-07-13T06:43:15.580264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda"
   ],
   "metadata": {
    "id": "4v1M4Mgaodvi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9f05ec74-9fc9-4dcc-b1d9-4a7091ecd3a9",
    "ExecuteTime": {
     "end_time": "2025-07-13T06:43:19.467884Z",
     "start_time": "2025-07-13T06:43:18.735559Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pathlib\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.document_loaders import PyPDFLoader"
   ],
   "metadata": {
    "collapsed": true,
    "id": "e-59QIjIj06-",
    "ExecuteTime": {
     "end_time": "2025-07-13T06:43:21.914981Z",
     "start_time": "2025-07-13T06:43:21.865874Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "## Papers download.",
   "metadata": {
    "id": "5GOkA-mjeYBs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "EMAIL = os.getenv(\"EMAIL\", 'trifonova.kate.s@gmail.com')\n",
    "BASE_URL = \"https://api.openalex.org/works\"\n",
    "\n",
    "\n",
    "def works_with_oa(query, first_n=1):\n",
    "    params = {\n",
    "        'filter': f'abstract.search:\"{query}\",best_open_version:published',\n",
    "        'per-page': first_n,\n",
    "        'select':'id,title,best_oa_location',\n",
    "        'mailto': EMAIL\n",
    "    }\n",
    "    papers = requests.get(BASE_URL, params=params).json()['results']\n",
    "    return papers\n",
    "\n",
    "def get_pdf(paper: dict, location: str) -> pathlib.Path | None:\n",
    "    url = paper.get('best_oa_location', {}).get('pdf_url')\n",
    "    paper_id = paper['id'].split('/')[-1]\n",
    "    pdf_dir = pathlib.Path(location)\n",
    "    pdf_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_path = pdf_dir / f\"{paper_id}.pdf\"\n",
    "\n",
    "    if not url:\n",
    "        print(f'No PDF URL for paper {paper_id}')\n",
    "        return None\n",
    "\n",
    "    # Skip existing files\n",
    "    if pdf_path.exists():\n",
    "        return pdf_path\n",
    "\n",
    "    # Attempt download with error handling\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update({'From': EMAIL})\n",
    "        try:\n",
    "            resp = session.get(url, stream=True)\n",
    "            resp.raise_for_status()\n",
    "        except requests.HTTPError as e:\n",
    "            print(f\"HTTPError downloading PDF for paper {paper_id}: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading PDF for paper {paper_id}: {e}\")\n",
    "            return None\n",
    "        # Write content in chunks\n",
    "        with open(pdf_path, 'wb') as f:\n",
    "            for chunk in resp.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    return pdf_path"
   ],
   "metadata": {
    "id": "IepIxS7NdQXx",
    "ExecuteTime": {
     "end_time": "2025-07-13T06:43:24.868783Z",
     "start_time": "2025-07-13T06:43:24.864388Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T07:22:32.296894Z",
     "start_time": "2025-07-13T07:22:32.223386Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ilyapetrovv@bk.ru\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "def has_text(pdf_path: Union[str, pathlib.Path], max_pages: int = 3, min_words: int = 50) -> bool:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    word_count = 0\n",
    "\n",
    "    for i, page in enumerate(reader.pages):\n",
    "        if i >= max_pages:\n",
    "            break\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            words = text.strip().split()\n",
    "            word_count += len(words)\n",
    "            if word_count >= min_words:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def load_and_split_docs(pdf_path: pathlib.Path) -> list:\n",
    "\n",
    "    if has_text(pdf_path):\n",
    "        loader = PyPDFLoader(str(pdf_path))\n",
    "    else:\n",
    "        loader = UnstructuredPDFLoader(str(pdf_path), strategy=\"ocr_only\")\n",
    "\n",
    "    docs = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = splitter.split_documents(docs)\n",
    "\n",
    "    for i, doc in enumerate(splits):\n",
    "        doc.metadata.update({\n",
    "            \"chunk_index\": i,\n",
    "            \"source_path\": str(pdf_path)\n",
    "        })\n",
    "\n",
    "    return splits"
   ],
   "metadata": {
    "id": "u3vG0s4SfEzv",
    "ExecuteTime": {
     "end_time": "2025-07-13T06:43:27.300438Z",
     "start_time": "2025-07-13T06:43:27.297228Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "def fetch_and_prepare(query: str, location: str = './pdfs', first_new: int = 3) -> list:\n",
    "\n",
    "    pdf_dir = pathlib.Path(location)\n",
    "    pdf_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Over-fetch candidates (3x) to allow skipping existing files\n",
    "    candidates = works_with_oa(query, first_new * 3)\n",
    "    new_docs = []\n",
    "\n",
    "    for paper in candidates:\n",
    "        paper_id = paper['id'].split('/')[-1]\n",
    "        pdf_path = pdf_dir / f\"{paper_id}.pdf\"\n",
    "        # Skip if already downloaded\n",
    "        if pdf_path.exists():\n",
    "            continue\n",
    "        # Download and prepare\n",
    "        downloaded = get_pdf(paper, location)\n",
    "        if downloaded:\n",
    "            new_docs.extend(load_and_split_docs(downloaded))\n",
    "        # Stop once we've got enough new ones\n",
    "        if len(new_docs) >= first_new:\n",
    "            break\n",
    "    return new_docs"
   ],
   "metadata": {
    "id": "iFVsjrCYdYJM",
    "ExecuteTime": {
     "end_time": "2025-07-13T06:43:32.579293Z",
     "start_time": "2025-07-13T06:43:32.575965Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "#the way to use rerun.\n",
    "\n",
    "# chain = rerun_pipeline('panthera leo', first_new=5)\n",
    "# answer = chain.invoke(\"question: What is a jaguar?\")\n",
    "# print(answer)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NjoFUlcSkQlF",
    "outputId": "8e6ab870-9842-4438-d90e-e899b299efa9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# what is the point of this?\n",
    "\n",
    "#what is the point of anything at all?\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#change its name.\n",
    "def function1(query: str, prompt: str):\n",
    "\n",
    "  embedding = OpenAIEmbeddings()\n",
    "\n",
    "  splits = fetch_and_prepare(query)\n",
    "\n",
    "  vectorstore = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "  retriever = vectorstore.as_retriever()\n",
    "\n",
    "  prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "  rag_chain = (\n",
    "      {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "      | prompt\n",
    "      | llm\n",
    "      | StrOutputParser()\n",
    "  )\n",
    "\n",
    "  answer = rag_chain.invoke({\"question\": prompt})\n",
    "\n",
    "  return answer"
   ],
   "metadata": {
    "id": "NYK3i1Vjrw_W",
    "ExecuteTime": {
     "end_time": "2025-07-12T22:19:12.743389Z",
     "start_time": "2025-07-12T22:19:12.736244Z"
    }
   },
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "id": "wGAtiVXjkDfc",
    "ExecuteTime": {
     "end_time": "2025-07-12T22:17:33.203611Z",
     "start_time": "2025-07-12T22:17:33.195942Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 29,
   "source": [
    "_default_first_new = 10\n",
    "_default_location = './pdfs'\n",
    "\n",
    "#since we have promt and llm defined inside of a function this pice of code doesnt work. we should move to classes approach.\n",
    "\n",
    "# def rerun_pipeline(query: str, first_new: int = _default_first_new, location: str = _default_location):\n",
    "#     splits = fetch_and_prepare(query, location, first_new)\n",
    "#     if not splits:\n",
    "#         raise ValueError(f\"No new document chunks retrieved for query '{query}'.\")\n",
    "#     vectorstore = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "#     retriever = vectorstore.as_retriever()\n",
    "#     return (\n",
    "#         {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "#         | prompt\n",
    "#         | llm\n",
    "#         | StrOutputParser()\n",
    "#     )\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "function1('starfish', prompt='What is a starfish?')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "qG1DQ5YNumJj",
    "ExecuteTime": {
     "end_time": "2025-07-13T10:16:52.729966Z",
     "start_time": "2025-07-13T10:16:52.701273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RAG:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vectorstore = None\n",
    "        self.vectorstore = Chroma(collection_name = \"bunch_of_docs\", embedding_function=OpenAIEmbeddings())\n",
    "        self.threshold = 0.30\n",
    "\n",
    "    @staticmethod\n",
    "    def has_text(pdf_path: str, max_pages: int = 3, min_words: int = 50) -> bool:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        word_count = 0\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            if i >= max_pages:\n",
    "                break\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                words = text.strip().split()\n",
    "                word_count += len(words)\n",
    "                if word_count >= min_words:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_and_prepare(query: str, location: str = './pdfs', first_new: int = 3) -> list:\n",
    "\n",
    "        pdf_dir = pathlib.Path(location)\n",
    "        pdf_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Over-fetch candidates (3x) to allow skipping existing files\n",
    "        candidates = works_with_oa(query, first_new * 3)\n",
    "        new_docs = []\n",
    "\n",
    "        for paper in candidates:\n",
    "            paper_id = paper['id'].split('/')[-1]\n",
    "            pdf_path = pdf_dir / f\"{paper_id}.pdf\"\n",
    "            # Skip if already downloaded\n",
    "            if pdf_path.exists():\n",
    "                continue\n",
    "            # Download and prepare\n",
    "            downloaded = get_pdf(paper, location)\n",
    "            if downloaded:\n",
    "                new_docs.extend(load_and_split_docs(downloaded))\n",
    "            # Stop once we've got enough new ones\n",
    "            if len(new_docs) >= first_new:\n",
    "                break\n",
    "        return new_docs\n",
    "\n",
    "    def load_ocr(self, pdf_path: str, puppy: str = None):\n",
    "        if self.has_text(pdf_path):\n",
    "            print(\"Text detected in PDF. Using fast strategy.\")\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "        else:\n",
    "            print(\"No text detected. Using OCR.\")\n",
    "            loader = UnstructuredPDFLoader(pdf_path, strategy=\"ocr_only\")\n",
    "\n",
    "        docs = loader.load()\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        splits = splitter.split_documents(docs)\n",
    "\n",
    "        for i, doc in enumerate(splits):\n",
    "            doc.metadata.update({\n",
    "                \"chunk_index\": i,\n",
    "                \"source_path\": str(pdf_path)\n",
    "            })\n",
    "            doc.metadata[\"specie\"] = puppy\n",
    "\n",
    "        self.vectorstore.add_documents(splits)\n",
    "\n",
    "    def chain(self, question):\n",
    "        if not self.vectorstore:\n",
    "            return \"Vector store not initialized.\"\n",
    "\n",
    "        def format_docs(docs):\n",
    "            return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "        results_with_scores = self.vectorstore.similarity_search_with_score(question, k=20)\n",
    "        filtered_docs = [doc for doc, score in results_with_scores if score <= self.threshold]\n",
    "\n",
    "        if not filtered_docs:\n",
    "            return \"booooooo, no docs found within the given threshold\"\n",
    "\n",
    "        # prompt\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"context\", \"question\"],\n",
    "            template=pathlib.Path('prompt.txt').read_text(encoding='utf8')\n",
    "        )\n",
    "\n",
    "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "        rag_chain = (\n",
    "            {\"context\": RunnableLambda(lambda _: format_docs(filtered_docs)),\n",
    "             \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        return rag_chain.invoke(question)\n",
    "\n",
    "    def query(self, question: str):\n",
    "\n",
    "        for puppy in question.split(\",\"):\n",
    "            print(type(puppy))\n",
    "            pdf_paths = self.fetch_and_prepare(query=puppy)\n",
    "\n",
    "            for pdf_path in pdf_paths:\n",
    "                print('eto put')\n",
    "                print(pdf_path)\n",
    "                self.load_ocr(pdf_path=pdf_path, puppy=puppy)\n",
    "        self.chain(question)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T10:16:53.634688Z",
     "start_time": "2025-07-13T10:16:53.599453Z"
    }
   },
   "cell_type": "code",
   "source": "RAG_obj = RAG()",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T10:16:57.089276Z",
     "start_time": "2025-07-13T10:16:53.721546Z"
    }
   },
   "cell_type": "code",
   "source": "RAG_obj.query(\"zebra, lion\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "No PDF URL for paper W1981920422\n",
      "HTTPError downloading PDF for paper W2101294025: 403 Client Error: Forbidden for url: https://www.annualreviews.org/doi/pdf/10.1146/annurev.genet.37.110801.143214\n",
      "eto put\n",
      "page_content='Behavioral/Systems/Cognitive\n",
      "FoxP2 Expression in Avian Vocal Learners and Non-Learners\n",
      "Sebastian Haesler,1* Kazuhiro Wada,2* A. Nshdejan,1 Edward E. Morrisey,4 Thierry Lints,3 Eric D. Jarvis,2 and\n",
      "Constance Scharff1\n",
      "1Max-Planck Institute for Molecular Genetics, 14195 Berlin, Germany, 2Department of Neurobiology, Duke University Medical Center, Durham, North\n",
      "Carolina 27710, 3City College of City University of New York, New York, New York 10031, and 4University of Pennsylvania, Philadelphia, Pennsylvania\n",
      "19104\n",
      "Most vertebrates communicate acoustically, but few, among them humans, dolphins and whales, bats, and three orders of birds, learn this\n",
      "trait. FOXP2 is the first gene linked to human speech and has been the target of positive selection during recent primate evolution. To test\n",
      "whether the expression pattern of FOXP2 is consistent with a role in learned vocal communication, we cloned zebra finch FoxP2 and its' metadata={'source': 'pdfs/W2166648354.pdf', 'page': 0, 'chunk_index': 0, 'source_path': 'pdfs/W2166648354.pdf'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Document' object has no attribute 'seek'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[40]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m RAG_obj.query(\u001B[33m\"\u001B[39m\u001B[33mzebra, lion\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[38]\u001B[39m\u001B[32m, line 110\u001B[39m, in \u001B[36mRAG.query\u001B[39m\u001B[34m(self, question)\u001B[39m\n\u001B[32m    108\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33meto put\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    109\u001B[39m         \u001B[38;5;28mprint\u001B[39m(pdf_path)\n\u001B[32m--> \u001B[39m\u001B[32m110\u001B[39m         \u001B[38;5;28mself\u001B[39m.load_ocr(pdf_path=pdf_path, puppy=puppy)\n\u001B[32m    111\u001B[39m \u001B[38;5;28mself\u001B[39m.chain(question)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[38]\u001B[39m\u001B[32m, line 49\u001B[39m, in \u001B[36mRAG.load_ocr\u001B[39m\u001B[34m(self, pdf_path, puppy)\u001B[39m\n\u001B[32m     48\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34mload_ocr\u001B[39m(\u001B[38;5;28mself\u001B[39m, pdf_path: \u001B[38;5;28mstr\u001B[39m, puppy: \u001B[38;5;28mstr\u001B[39m = \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.has_text(pdf_path):\n\u001B[32m     50\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mText detected in PDF. Using fast strategy.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     51\u001B[39m         loader = PyPDFLoader(pdf_path)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[38]\u001B[39m\u001B[32m, line 10\u001B[39m, in \u001B[36mRAG.has_text\u001B[39m\u001B[34m(pdf_path, max_pages, min_words)\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34mhas_text\u001B[39m(pdf_path: \u001B[38;5;28mstr\u001B[39m, max_pages: \u001B[38;5;28mint\u001B[39m = \u001B[32m3\u001B[39m, min_words: \u001B[38;5;28mint\u001B[39m = \u001B[32m50\u001B[39m) -> \u001B[38;5;28mbool\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m     reader = PdfReader(pdf_path)\n\u001B[32m     11\u001B[39m     word_count = \u001B[32m0\u001B[39m\n\u001B[32m     12\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i, page \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(reader.pages):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/DendoLLM/lib/python3.12/site-packages/PyPDF2/_reader.py:319\u001B[39m, in \u001B[36mPdfReader.__init__\u001B[39m\u001B[34m(self, stream, strict, password)\u001B[39m\n\u001B[32m    317\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(stream, \u001B[33m\"\u001B[39m\u001B[33mrb\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fh:\n\u001B[32m    318\u001B[39m         stream = BytesIO(fh.read())\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m \u001B[38;5;28mself\u001B[39m.read(stream)\n\u001B[32m    320\u001B[39m \u001B[38;5;28mself\u001B[39m.stream = stream\n\u001B[32m    322\u001B[39m \u001B[38;5;28mself\u001B[39m._override_encryption = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/DendoLLM/lib/python3.12/site-packages/PyPDF2/_reader.py:1414\u001B[39m, in \u001B[36mPdfReader.read\u001B[39m\u001B[34m(self, stream)\u001B[39m\n\u001B[32m   1413\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34mread\u001B[39m(\u001B[38;5;28mself\u001B[39m, stream: StreamType) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1414\u001B[39m     \u001B[38;5;28mself\u001B[39m._basic_validation(stream)\n\u001B[32m   1415\u001B[39m     \u001B[38;5;28mself\u001B[39m._find_eof_marker(stream)\n\u001B[32m   1416\u001B[39m     startxref = \u001B[38;5;28mself\u001B[39m._find_startxref_pos(stream)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/DendoLLM/lib/python3.12/site-packages/PyPDF2/_reader.py:1453\u001B[39m, in \u001B[36mPdfReader._basic_validation\u001B[39m\u001B[34m(self, stream)\u001B[39m\n\u001B[32m   1451\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[34m_basic_validation\u001B[39m(\u001B[38;5;28mself\u001B[39m, stream: StreamType) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1452\u001B[39m     \u001B[38;5;66;03m# start at the end:\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1453\u001B[39m     stream.seek(\u001B[32m0\u001B[39m, os.SEEK_END)\n\u001B[32m   1454\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream.tell():\n\u001B[32m   1455\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m EmptyFileError(\u001B[33m\"\u001B[39m\u001B[33mCannot read an empty file\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/miniconda3/envs/DendoLLM/lib/python3.12/site-packages/pydantic/main.py:991\u001B[39m, in \u001B[36mBaseModel.__getattr__\u001B[39m\u001B[34m(self, item)\u001B[39m\n\u001B[32m    988\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__getattribute__\u001B[39m(item)  \u001B[38;5;66;03m# Raises AttributeError if appropriate\u001B[39;00m\n\u001B[32m    989\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    990\u001B[39m     \u001B[38;5;66;03m# this is the current error\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m991\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m).\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[33m object has no attribute \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mitem\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[33m'\u001B[39m)\n",
      "\u001B[31mAttributeError\u001B[39m: 'Document' object has no attribute 'seek'"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
